{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition using Recurrent Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name entity recognition is the task to label nouns in sentences with their categories. For example, words like Paris, London or Tokyo share the label \"Location;\" European Comission, BBC, Foreign Ministry are all \"Organizations.\" These tags can be helpful for other system functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "To build models for name enitity recognition task, we need to first convert words and corresponding name entity tags in sentences to numbers. Then, we need to make all sentences the same length by adding zeros to the end of sentences shorter than the longest sentence in the dataset (aka padding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build Corpus & Tags dictionary\n",
    "To convert words to numbers, we need to <br>\n",
    "(1) read in the sentences and name entities in the training dataset.<br>\n",
    "(2) break sentences into lists of words (tokenization) and tags.<br>\n",
    "(3) create a dictionary \"Corpus\" to map every word to a number, plus an \"UNK\" token for unknown words and another dictionary called \"Tags\" to store mapping of all name entity tags and the corresponding number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_text = open('./conll2003/train.txt')\n",
    "#Set to store unique word and name entitiy tags\n",
    "Corpus_Train = set()\n",
    "NE_tag_Train = set()\n",
    "\n",
    "for line in Train_text:    \n",
    "    splits = line.split(' ')\n",
    "    splits[-1] = splits[-1].rstrip(\"\\n\")\n",
    "    if len(splits)  > 1:\n",
    "        word = splits[0]\n",
    "        NE_tag = splits[-1]\n",
    "        Corpus_Train.add(word)\n",
    "        NE_tag_Train.add(NE_tag)\n",
    "Train_text.close()\n",
    "\n",
    "#Create corpus and dictionary\n",
    "AllWords_l = sorted(Corpus_Train)\n",
    "AllNE_tag_l = sorted(NE_tag_Train)\n",
    "\n",
    "Corpus = dict()\n",
    "Tags = dict()\n",
    "for i in range(len(AllWords_l)):\n",
    "    Corpus[AllWords_l[i]]=i\n",
    "Corpus['UNK'] = len(AllWords_l)\n",
    "\n",
    "for j in range(len(AllNE_tag_l)):\n",
    "    Tags[AllNE_tag_l[j]]=j\n",
    "\n",
    "Tags['UNK'] = len(AllNE_tag_l)\n",
    "num_labels = len(Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The resulted dictionary of name entity tags looks like:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 0,\n",
       " 'B-MISC': 1,\n",
       " 'B-ORG': 2,\n",
       " 'B-PER': 3,\n",
       " 'I-LOC': 4,\n",
       " 'I-MISC': 5,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 7,\n",
       " 'O': 8,\n",
       " 'UNK': 9}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The resulted dictionary of name entity tags looks like:\")\n",
    "Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Turn sentenses to list of tokens and name entity tags\n",
    "Below is the function for tokenizing sentences in the dataset, which can be used on training set, validation set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentenceToList(filePath):\n",
    "    Train_text = open(filePath)\n",
    "    \n",
    "    #list of lists of sentences\n",
    "    SenteceCollect = list()\n",
    "    NE_tagCollect = list()\n",
    "\n",
    "    #list to store individual sentences\n",
    "    Sentence_l = list()\n",
    "    NE_tag_l = list()\n",
    "\n",
    "    #max length\n",
    "    max_sent_len = 0\n",
    "\n",
    "    for line in Train_text:\n",
    "    \n",
    "        splits = line.split(' ')\n",
    "        splits[-1] = splits[-1].rstrip(\"\\n\")\n",
    "    \n",
    "        if len(splits)  > 1:\n",
    "            word = splits[0]\n",
    "            NE_tag = splits[-1]\n",
    "            Sentence_l.append(word)\n",
    "            NE_tag_l.append(NE_tag)\n",
    "        else:\n",
    "            SenteceCollect.append(Sentence_l)\n",
    "            if len(Sentence_l) > max_sent_len:\n",
    "                max_sent_len = len(Sentence_l) \n",
    "            NE_tagCollect.append(NE_tag_l)\n",
    "            Sentence_l =list()\n",
    "            NE_tag_l = list()\n",
    "    Train_text.close()\n",
    "    return SenteceCollect, NE_tagCollect, max_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two sentenses in the train set tokenized:\n",
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn']]\n",
      "Name entity abels of the first two sentenses in the train set:\n",
      "[['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER']]\n",
      "Max sentence length of train set:\n",
      "113\n",
      "Max sentence length of Validation set:\n",
      "109\n",
      "Max sentence length of Test set:\n",
      "124\n"
     ]
    }
   ],
   "source": [
    "Train_Sent_l, Train_Tag_l, Train_MaxLen = SentenceToList('./conll2003/train.txt')\n",
    "Val_Sent_l, Val_Tag_l, Val_MaxLen = SentenceToList('./conll2003/valid.txt')\n",
    "Test_Sent_l, Test_Tag_l, Test_MaxLen = SentenceToList('./conll2003/test.txt')\n",
    "\n",
    "print(\"The first two sentenses in the train set tokenized:\")\n",
    "print(Train_Sent_l[1:3])\n",
    "print(\"Name entity abels of the first two sentenses in the train set:\")\n",
    "print(Train_Tag_l[1:3])\n",
    "\n",
    "print(\"Max sentence length of train set:\")\n",
    "print(Train_MaxLen)\n",
    "\n",
    "print(\"Max sentence length of Validation set:\")\n",
    "print(Val_MaxLen)\n",
    "print(\"Max sentence length of Test set:\")\n",
    "print(Test_MaxLen)\n",
    "\n",
    "Max_Len = Test_MaxLen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Map words and tags to number\n",
    "The function below maps tokens to indices in the Corpus and Tags Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MapTokenInd(sentences, TokenDic):\n",
    "    SentencesTokenInd = []\n",
    "    for sentence in sentences:\n",
    "        TokenIndices = []\n",
    "        for token in sentence:\n",
    "            #token = token.lower()\n",
    "            if token in TokenDic:\n",
    "                TokenIdx = TokenDic[token]\n",
    "            else:\n",
    "                TokenIdx = TokenDic['UNK']\n",
    "            TokenIndices.append(TokenIdx)\n",
    "        SentencesTokenInd.append(TokenIndices)\n",
    "    return SentencesTokenInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two sentenses in the training set mapped to index has become:\n",
      "[[6419, 20820, 7228, 14821, 22699, 14672, 5083, 18389, 124], [10720, 4910]]\n",
      "Name entity labels mapped to label index of the first two sentenses in the train set:\n",
      "[[2, 8, 1, 8, 8, 8, 1, 8, 8], [3, 7]]\n"
     ]
    }
   ],
   "source": [
    "#Training set\n",
    "Train_Sent_Ind_l = MapTokenInd(Train_Sent_l, Corpus)\n",
    "Train_Tag_Ind_l = MapTokenInd(Train_Tag_l, Tags)\n",
    "#Validation set\n",
    "Val_Sent_Ind_l = MapTokenInd(Val_Sent_l, Corpus)\n",
    "Val_Tag_Ind_l = MapTokenInd(Val_Tag_l, Tags)\n",
    "#Test Set\n",
    "Test_Sent_Ind_l = MapTokenInd(Test_Sent_l, Corpus)\n",
    "Test_Tag_Ind_l = MapTokenInd(Test_Tag_l, Tags)\n",
    "\n",
    "\n",
    "print(\"The first two sentenses in the training set mapped to index has become:\")\n",
    "print(Train_Sent_Ind_l[1:3])\n",
    "print(\"Name entity labels mapped to label index of the first two sentenses in the train set:\")\n",
    "print(Train_Tag_Ind_l[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: padding\n",
    "To make all sentences in the dataset the same length to put build models, we add zeros to the end of each sentece (called padding) to make them all the same length of the longest sentence, in this case, 124 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding, the first two sentences and tags of the training set become:\n",
      "[[ 6419 20820  7228 14821 22699 14672  5083 18389   124     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [10720  4910     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "[[ 6419 20820  7228 14821 22699 14672  5083 18389   124     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [10720  4910     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "#Padding\n",
    "Train_Sent_Padded = pad_sequences(Train_Sent_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Train_Tag_Padded = pad_sequences(Train_Tag_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Train_Labels_OneHot = [to_categorical(i, num_classes=num_labels) for i in Train_Tag_Padded]\n",
    "\n",
    "\n",
    "Val_Sent_Padded = pad_sequences(Val_Sent_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Val_Tag_Padded = pad_sequences(Val_Tag_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Val_Labels_OneHot = [to_categorical(j, num_classes=num_labels) for j in Val_Tag_Padded]\n",
    "\n",
    "Test_Sent_Padded = pad_sequences(Test_Sent_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Test_Tag_Padded = pad_sequences(Test_Tag_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Test_Labels_OneHot = [to_categorical(k, num_classes=num_labels) for k in Test_Tag_Padded]\n",
    "\n",
    "\n",
    "print(\"After padding, the first two sentences and tags of the training set become:\")\n",
    "print(Train_Sent_Padded[1:3])\n",
    "print(Train_Sent_Padded[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Buildup\n",
    "Here, we use tensorflow to build a sequence model containing an embedding layer, a bi-directional Long-Short Term Memory (LSTM) layer and a dense layer. The structures (layers in the model) and other hyperparameters (e.g. learning rate, activation function, dropout rate, regularization) are all hyper parameters which can be tunned to optimize evaluation matrics using the training and validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(layers.Embedding(input_dim=len(Corpus), output_dim=64, \n",
    "                        input_length=Max_Len))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(layers.Bidirectional(layers.LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "    # Add LSTM\n",
    "    #model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(layers.TimeDistributed(layers.Dense(num_labels, activation=\"softmax\")))\n",
    "\n",
    "    #Optimiser \n",
    "    adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 124, 64)           1512000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 124, 128)         66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 124, 10)          1290      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,579,338\n",
      "Trainable params: 1,579,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/halfmoonliu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure summary:\")\n",
    "model = get_bilstm_lstm_model()\n",
    "#plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model training\n",
    "We use the above mentioned model structure to train on the training dataset. Epochs (times the model runs through all the training data) and batch size (during an epoch, training data are processed on batch at a time untill the whole training set is processed) are to be adjusted to maximize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(5):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=500, verbose=1, epochs=1)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 43s 1s/step - loss: 1.0807 - accuracy: 0.8647\n",
      "30/30 [==============================] - 33s 1s/step - loss: 0.1584 - accuracy: 0.9465\n",
      "30/30 [==============================] - 33s 1s/step - loss: 0.1167 - accuracy: 0.9735\n",
      "30/30 [==============================] - 31s 1s/step - loss: 0.1016 - accuracy: 0.9773\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.0936 - accuracy: 0.9784\n"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "results = pd.DataFrame()\n",
    "results['with_add_lstm'] = train_model(Train_Sent_Padded, np.array(Train_Labels_OneHot), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Validation\n",
    "Hyperparameters can be tuned to maximize the model performance on the validation set. The performance on the training and validation set both serve as reference for hyperparameter tuning (eg. bias-variance issue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.976360/\n"
     ]
    }
   ],
   "source": [
    "Val_pred = model.predict(Val_Sent_Padded)\n",
    "\n",
    "Val_pred = np.argmax(Val_pred, axis=2)\n",
    "\n",
    "Val_Labels = np.argmax(Val_Labels_OneHot, axis=2)\n",
    "\n",
    "Val_Accuracy = (Val_pred == Val_Labels).mean()\n",
    "\n",
    "print(\"Validation Accuracy: {:8f}/\".format(Val_Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the Model with Unseen Test Data\n",
    "Finally, we use a held-out data set to test model performance. This is to avoid overfitting due to hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97909881/\n"
     ]
    }
   ],
   "source": [
    "Test_pred = model.predict(Test_Sent_Padded)\n",
    "\n",
    "Test_pred = np.argmax(Test_pred, axis=2)\n",
    "\n",
    "Test_Labels = np.argmax(Test_Labels_OneHot, axis=2)\n",
    "\n",
    "Test_Accuracy = (Test_pred == Test_Labels).mean()\n",
    "\n",
    "print(\"Test Accuracy: {:.8f}/\".format(Test_Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performances on sentences of different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences with less than or equal to 10 tokens in the test set:\n",
      "2295\n",
      "Number of sentences with more than 10 tokens in the test set:\n",
      "1388\n"
     ]
    }
   ],
   "source": [
    "#Find cutpoint for long and short sentences in the test set\n",
    "\n",
    "Test_sent_l_dic = dict()\n",
    "for sentence in Test_Sent_l:\n",
    "    if len(sentence) not in Test_sent_l_dic:\n",
    "        Test_sent_l_dic[len(sentence)] = 1\n",
    "    else:\n",
    "        Test_sent_l_dic[len(sentence)] += 1\n",
    "\n",
    "num_short_sentences = 0\n",
    "num_long_sentences = 0\n",
    "\n",
    "for i in range(1, Test_MaxLen):\n",
    "    if i <11:\n",
    "        num_short_sentences += Test_sent_l_dic[i]\n",
    "    else:\n",
    "        try:\n",
    "            num_long_sentences += Test_sent_l_dic[i]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(\"Number of sentences with less than or equal to 10 tokens in the test set:\")\n",
    "print(num_short_sentences)\n",
    "print(\"Number of sentences with more than 10 tokens in the test set:\")\n",
    "print(num_long_sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentToListCat(filePath):\n",
    "    Train_text = open(filePath)\n",
    "    \n",
    "    #list of lists of sentences\n",
    "    SentCollectShort = list()\n",
    "    SentCollectLong = list()\n",
    "    NE_tagCollectShort = list()\n",
    "    NE_tagCollectLong = list()\n",
    "\n",
    "\n",
    "    #list to store individual sentences\n",
    "    Sentence_l = list()\n",
    "    NE_tag_l = list()\n",
    "\n",
    "    #max length\n",
    "    max_sent_len = 0\n",
    "\n",
    "    for line in Train_text:\n",
    "    \n",
    "        splits = line.split(' ')\n",
    "        splits[-1] = splits[-1].rstrip(\"\\n\")\n",
    "    \n",
    "        if len(splits)  > 1:\n",
    "            word = splits[0]\n",
    "            NE_tag = splits[-1]\n",
    "            Sentence_l.append(word)\n",
    "            NE_tag_l.append(NE_tag)\n",
    "        else:\n",
    "            \n",
    "            #separating long and short sentences\n",
    "            if len(Sentence_l)>10:\n",
    "                SentCollectLong.append(Sentence_l)\n",
    "                NE_tagCollectLong.append(NE_tag_l)                \n",
    "            else:\n",
    "                SentCollectShort.append(Sentence_l)\n",
    "                NE_tagCollectShort.append(NE_tag_l)\n",
    "\n",
    "                \n",
    "            Sentence_l =list()\n",
    "            NE_tag_l = list()\n",
    "    Train_text.close()\n",
    "\n",
    "    return SentCollectLong, NE_tagCollectLong, SentCollectShort, NE_tagCollectShort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat sentencese\n",
    "TestSentLong_l, TestTagLong_l, TestSentShort_l, TestTagShort_l = SentToListCat('./conll2003/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map to token\n",
    "TestSentLong_Ind_l = MapTokenInd(TestSentLong_l, Corpus)\n",
    "TestTagLong_Ind_l = MapTokenInd(TestTagLong_l, Tags)\n",
    "TestSentShort_Ind_l = MapTokenInd(TestSentShort_l, Corpus)\n",
    "TestTagShrot_Ind_l = MapTokenInd(TestTagShort_l, Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "TestSentLong_Padded = pad_sequences(TestSentLong_Ind_l, maxlen=Max_Len, padding='post')\n",
    "TestTagLong_Padded = pad_sequences(TestTagLong_Ind_l, maxlen=Max_Len, padding='post')\n",
    "TestLabelsLong_OneHot = [to_categorical(k, num_classes=num_labels) for k in TestTagLong_Padded]\n",
    "#padding\n",
    "TestSentShort_Padded = pad_sequences(TestSentShort_Ind_l, maxlen=Max_Len, padding='post')\n",
    "TestTagShort_Padded = pad_sequences(TestTagShrot_Ind_l, maxlen=Max_Len, padding='post')\n",
    "TestLabelsShort_OneHot = [to_categorical(k, num_classes=num_labels) for k in TestTagShort_Padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Long Sentences: 0.96715553/\n"
     ]
    }
   ],
   "source": [
    "TestPredLong = model.predict(TestSentLong_Padded)\n",
    "\n",
    "TestPredLong = np.argmax(TestPredLong, axis=2)\n",
    "\n",
    "TestLabelLong = np.argmax(TestLabelsLong_OneHot, axis=2)\n",
    "\n",
    "TestAccLong = (TestPredLong == TestLabelLong).mean()\n",
    "\n",
    "print(\"Test Accuracy Long Sentences: {:.8f}/\".format(TestAccLong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Short Sentences: 0.98632722/\n"
     ]
    }
   ],
   "source": [
    "TestPredShort = model.predict(TestSentShort_Padded)\n",
    "\n",
    "TestPredShort = np.argmax(TestPredShort, axis=2)\n",
    "\n",
    "TestLabelShort = np.argmax(TestLabelsShort_OneHot, axis=2)\n",
    "\n",
    "TestAccShort = (TestPredShort == TestLabelShort).mean()\n",
    "\n",
    "print(\"Test Accuracy Short Sentences: {:.8f}/\".format(TestAccShort))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142â€“147. Association for Computational Linguistics. \n",
    "2. Jason PC Chiu and Eric Nichols. 2015. Named entity recognition with bidirectional lstm-cnns. arXiv preprint arXiv:1511.08308."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
