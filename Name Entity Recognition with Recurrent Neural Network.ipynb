{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition using Recurrent Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name entity recognition is the task to label nouns in sentences with their categories. For example, words like Paris, London or Tokyo share the label \"Location;\" European Comission, BBC, Foreign Ministry are all \"Organizations.\" These tags can be helpful for other system functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/halfmoonliu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "To build models for name enitity recognition task, we need to first convert words and corresponding name entity tags in sentences to numbers. Then, we need to make all sentences the same length by adding zeros to the end of sentences shorter than the longest sentence in the dataset (aka padding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build Corpus & Tag dictionary\n",
    "To convert words to numbers, we need to <br>\n",
    "(1) read in the sentences and name entities in the training dataset.<br>\n",
    "(2) break sentences into lists of words (tokenization) and tags.<br>\n",
    "(3) create a dictionary \"Corpus\" to map every word to a number, plus an \"UNK\" token for unknown words and another dictionary called \"Tags\" to store mapping of all name entity tags and the corresponding number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_text = open('./conll2003/train.txt')\n",
    "#Set to store unique word and name entitiy tags\n",
    "Corpus_Train = set()\n",
    "NE_tag_Train = set()\n",
    "\n",
    "for line in Train_text:    \n",
    "    splits = line.split(' ')\n",
    "    splits[-1] = splits[-1].rstrip(\"\\n\")\n",
    "    if len(splits)  > 1:\n",
    "        word = splits[0]\n",
    "        NE_tag = splits[-1]\n",
    "        Corpus_Train.add(word)\n",
    "        NE_tag_Train.add(NE_tag)\n",
    "Train_text.close()\n",
    "\n",
    "#Create corpus and dictionary\n",
    "AllWords_l = sorted(Corpus_Train)\n",
    "AllNE_tag_l = sorted(NE_tag_Train)\n",
    "\n",
    "Corpus = dict()\n",
    "Tags = dict()\n",
    "for i in range(len(AllWords_l)):\n",
    "    Corpus[AllWords_l[i]]=i\n",
    "Corpus['UNK'] = len(AllWords_l)\n",
    "\n",
    "for j in range(len(AllNE_tag_l)):\n",
    "    Tags[AllNE_tag_l[j]]=j\n",
    "\n",
    "Tags['UNK'] = len(AllNE_tag_l)\n",
    "num_labels = len(Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The resulted dictionary of name entity tags looks like:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 0,\n",
       " 'B-MISC': 1,\n",
       " 'B-ORG': 2,\n",
       " 'B-PER': 3,\n",
       " 'I-LOC': 4,\n",
       " 'I-MISC': 5,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 7,\n",
       " 'O': 8,\n",
       " 'UNK': 9}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The resulted dictionary of name entity tags looks like:\")\n",
    "Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Turn sentenses to list of tokens and name entity tags.\n",
    "Below is the function for tokenizing sentences in the dataset, which can be used on training set, validation set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentenceToList(filePath):\n",
    "    Train_text = open(filePath)\n",
    "    \n",
    "    #list of lists of sentences\n",
    "    SenteceCollect = list()\n",
    "    NE_tagCollect = list()\n",
    "\n",
    "    #list to store individual sentences\n",
    "    Sentence_l = list()\n",
    "    NE_tag_l = list()\n",
    "\n",
    "    #max length\n",
    "    max_sent_len = 0\n",
    "\n",
    "    for line in Train_text:\n",
    "    \n",
    "        splits = line.split(' ')\n",
    "        splits[-1] = splits[-1].rstrip(\"\\n\")\n",
    "    \n",
    "        if len(splits)  > 1:\n",
    "            word = splits[0]\n",
    "            NE_tag = splits[-1]\n",
    "            Sentence_l.append(word)\n",
    "            NE_tag_l.append(NE_tag)\n",
    "        else:\n",
    "            SenteceCollect.append(Sentence_l)\n",
    "            if len(Sentence_l) > max_sent_len:\n",
    "                max_sent_len = len(Sentence_l) \n",
    "            NE_tagCollect.append(NE_tag_l)\n",
    "            Sentence_l =list()\n",
    "            NE_tag_l = list()\n",
    "    Train_text.close()\n",
    "    return SenteceCollect, NE_tagCollect, max_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two sentenses in the train set tokenized:\n",
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn']]\n",
      "Name entity abels of the first two sentenses in the train set:\n",
      "[['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER']]\n",
      "Max senten length of train set:\n",
      "113\n",
      "Max senten length of Validation set:\n",
      "109\n",
      "Max senten length of Test set:\n",
      "124\n"
     ]
    }
   ],
   "source": [
    "Train_Sent_l, Train_Tag_l, Train_MaxLen = SentenceToList('./conll2003/train.txt')\n",
    "Val_Sent_l, Val_Tag_l, Val_MaxLen = SentenceToList('./conll2003/valid.txt')\n",
    "Test_Sent_l, Test_Tag_l, Test_MaxLen = SentenceToList('./conll2003/test.txt')\n",
    "\n",
    "print(\"The first two sentenses in the train set tokenized:\")\n",
    "print(Train_Sent_l[1:3])\n",
    "print(\"Name entity abels of the first two sentenses in the train set:\")\n",
    "print(Train_Tag_l[1:3])\n",
    "\n",
    "print(\"Max senten length of train set:\")\n",
    "print(Train_MaxLen)\n",
    "\n",
    "print(\"Max senten length of Validation set:\")\n",
    "print(Val_MaxLen)\n",
    "print(\"Max senten length of Test set:\")\n",
    "print(Test_MaxLen)\n",
    "\n",
    "Max_Len = Test_MaxLen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Map words and tags to numbe.\n",
    "The function below maps tokens to indices in the Corpus and Tags Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MapTokenInd(sentences, TokenDic):\n",
    "    SentencesTokenInd = []\n",
    "    for sentence in sentences:\n",
    "        TokenIndices = []\n",
    "        for token in sentence:\n",
    "            #token = token.lower()\n",
    "            if token in TokenDic:\n",
    "                TokenIdx = TokenDic[token]\n",
    "            else:\n",
    "                TokenIdx = TokenDic['UNK']\n",
    "            TokenIndices.append(TokenIdx)\n",
    "        SentencesTokenInd.append(TokenIndices)\n",
    "    return SentencesTokenInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two sentenses in the training set mapped to index has become:\n",
      "[[6419, 20820, 7228, 14821, 22699, 14672, 5083, 18389, 124], [10720, 4910]]\n",
      "Name entity labels mapped to label index of the first two sentenses in the train set:\n",
      "[[2, 8, 1, 8, 8, 8, 1, 8, 8], [3, 7]]\n"
     ]
    }
   ],
   "source": [
    "#Training set\n",
    "Train_Sent_Ind_l = MapTokenInd(Train_Sent_l, Corpus)\n",
    "Train_Tag_Ind_l = MapTokenInd(Train_Tag_l, Tags)\n",
    "#Validation set\n",
    "Val_Sent_Ind_l = MapTokenInd(Val_Sent_l, Corpus)\n",
    "Val_Tag_Ind_l = MapTokenInd(Val_Tag_l, Tags)\n",
    "#Test Set\n",
    "Test_Sent_Ind_l = MapTokenInd(Test_Sent_l, Corpus)\n",
    "Test_Tag_Ind_l = MapTokenInd(Test_Tag_l, Tags)\n",
    "\n",
    "\n",
    "print(\"The first two sentenses in the training set mapped to index has become:\")\n",
    "print(Train_Sent_Ind_l[1:3])\n",
    "print(\"Name entity labels mapped to label index of the first two sentenses in the train set:\")\n",
    "print(Train_Tag_Ind_l[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: padding\n",
    "To make all sentences in the dataset the same length to put build models, we add zeros to the end of each sentece (called padding) to make them all the same length of the longest sentence, in this case, 124 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding, the first two sentences and tags of the training set become:\n",
      "[[ 6419 20820  7228 14821 22699 14672  5083 18389   124     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [10720  4910     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "[[ 6419 20820  7228 14821 22699 14672  5083 18389   124     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [10720  4910     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "#Padding\n",
    "Train_Sent_Padded = pad_sequences(Train_Sent_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Train_Tag_Padded = pad_sequences(Train_Tag_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Train_Labels_OneHot = [to_categorical(i, num_classes=num_labels) for i in Train_Tag_Padded]\n",
    "\n",
    "\n",
    "Val_Sent_Padded = pad_sequences(Val_Sent_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Val_Tag_Padded = pad_sequences(Val_Tag_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Val_Labels_OneHot = [to_categorical(j, num_classes=num_labels) for j in Val_Tag_Padded]\n",
    "\n",
    "Test_Sent_Padded = pad_sequences(Test_Sent_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Test_Tag_Padded = pad_sequences(Test_Tag_Ind_l, maxlen=Max_Len, padding='post')\n",
    "Test_Labels_OneHot = [to_categorical(k, num_classes=num_labels) for k in Test_Tag_Padded]\n",
    "\n",
    "\n",
    "print(\"After padding, the first two sentences and tags of the training set become:\")\n",
    "print(Train_Sent_Padded[1:3])\n",
    "print(Train_Sent_Padded[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Buildup\n",
    "Here, we use tensorflow to build a sequence model containing an embedding layer, a bi-directional Long-Short Term Memory (LSTM) layer and a dense layer. The structures (layers in the model) and other hyperparameters (e.g. learning rate, activation function, dropout rate, regularization) are all hyper parameters which can be tunned to optimize evaluation matrics using the training and validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(layers.Embedding(input_dim=len(Corpus), output_dim=64, \n",
    "                        input_length=Max_Len))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(layers.Bidirectional(layers.LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "    # Add LSTM\n",
    "    #model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(layers.TimeDistributed(layers.Dense(num_labels, activation=\"softmax\")))\n",
    "\n",
    "    #Optimiser \n",
    "    adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 124, 64)           1512000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 124, 128)          66048     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 124, 10)           1290      \n",
      "=================================================================\n",
      "Total params: 1,579,338\n",
      "Trainable params: 1,579,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure summary:\")\n",
    "model = get_bilstm_lstm_model()\n",
    "#plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model training\n",
    "We use the above mentioned model structure to train on the training dataset. Epochs (times the model runs through all the training data) and batch size (during an epoch, training data are processed on batch at a time untill the whole training set is processed) are to be adjusted to maximize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(5):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=500, verbose=1, epochs=1)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples\n",
      "14987/14987 [==============================] - 31s 2ms/sample - loss: 1.0011 - accuracy: 0.8662\n",
      "Train on 14987 samples\n",
      "14987/14987 [==============================] - 25s 2ms/sample - loss: 0.1738 - accuracy: 0.9579\n",
      "Train on 14987 samples\n",
      "14987/14987 [==============================] - 27s 2ms/sample - loss: 0.1166 - accuracy: 0.9753\n",
      "Train on 14987 samples\n",
      "14987/14987 [==============================] - 29s 2ms/sample - loss: 0.1028 - accuracy: 0.9776\n",
      "Train on 14987 samples\n",
      "14987/14987 [==============================] - 29s 2ms/sample - loss: 0.0957 - accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "results = pd.DataFrame()\n",
    "results['with_add_lstm'] = train_model(Train_Sent_Padded, np.array(Train_Labels_OneHot), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Validation\n",
    "Hyper parameters are can be adjusted to maximize the model performance on the validation set. The performance on the training and validation set both serve as reference for hyper parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.976963/\n"
     ]
    }
   ],
   "source": [
    "Val_pred = model.predict(Val_Sent_Padded)\n",
    "\n",
    "Val_pred = np.argmax(Val_pred, axis=2)\n",
    "\n",
    "Val_Labels = np.argmax(Val_Labels_OneHot, axis=2)\n",
    "\n",
    "Val_Accuracy = (Val_pred == Val_Labels).mean()\n",
    "\n",
    "print(\"Validation Accuracy: {:8f}/\".format(Val_Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the Model with Unseen Test Data\n",
    "Finally, we use a held-out data set to test model performance. This is to avoid overfitting due to hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97962418/\n"
     ]
    }
   ],
   "source": [
    "Test_pred = model.predict(Test_Sent_Padded)\n",
    "\n",
    "Test_pred = np.argmax(Test_pred, axis=2)\n",
    "\n",
    "Test_Labels = np.argmax(Test_Labels_OneHot, axis=2)\n",
    "\n",
    "Test_Accuracy = (Test_pred == Test_Labels).mean()\n",
    "\n",
    "print(\"Test Accuracy: {:.8f}/\".format(Test_Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142â€“147. Association for Computational Linguistics. <br>\n",
    "Jason PC Chiu and Eric Nichols. 2015. Named entity recognition with bidirectional lstm-cnns. arXiv preprint arXiv:1511.08308."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
